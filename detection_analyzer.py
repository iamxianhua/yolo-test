#!/usr/bin/env python3
"""
YOLO æ£€æµ‹ç»“æœåˆ†æå·¥å…·
é…åˆ yolo-detection-analysis skill ä½¿ç”¨
"""

import json
import time
from datetime import datetime


class DetectionAnalyzer:
    """æ£€æµ‹ç»“æœåˆ†æå™¨"""

    def __init__(self):
        self.results_history = []

    def analyze_detection_result(self, boxes, confidences, classIDs, labels, inference_time=None):
        """
        åˆ†æå•æ¬¡æ£€æµ‹ç»“æœ

        Args:
            boxes: æ£€æµ‹æ¡†åˆ—è¡¨
            confidences: ç½®ä¿¡åº¦åˆ—è¡¨
            classIDs: ç±»åˆ« ID åˆ—è¡¨
            labels: ç±»åˆ«æ ‡ç­¾åˆ—è¡¨
            inference_time: æ¨ç†æ—¶é—´ï¼ˆç§’ï¼‰
        """
        print("\n" + "=" * 60)
        print("YOLO æ£€æµ‹ç»“æœåˆ†ææŠ¥å‘Š")
        print("=" * 60)
        print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        # åŸºæœ¬ç»Ÿè®¡
        total_detections = len(boxes)
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0

        print("[åŸºæœ¬ä¿¡æ¯]")
        print(f"  æ£€æµ‹æ¡†æ•°é‡: {total_detections}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2%}")

        if inference_time:
            fps = 1.0 / inference_time if inference_time > 0 else 0
            print(f"  æ¨ç†æ—¶é—´: {inference_time*1000:.2f} ms")
            print(f"  FPS: {fps:.2f}")

        # ç±»åˆ«åˆ†å¸ƒ
        class_distribution = {}
        for cid in classIDs:
            class_name = labels[cid]
            class_distribution[class_name] = class_distribution.get(class_name, 0) + 1

        print(f"\n[æ£€æµ‹ç±»åˆ«åˆ†å¸ƒ]")
        for class_name, count in sorted(class_distribution.items(), key=lambda x: x[1], reverse=True):
            print(f"  {class_name}: {count} ä¸ª")

        # ç½®ä¿¡åº¦åˆ†æ
        confidence_levels = {
            'high': [],      # > 0.8
            'medium': [],    # 0.5-0.8
            'low': []        # < 0.5
        }

        for i, conf in enumerate(confidences):
            if conf > 0.8:
                confidence_levels['high'].append((i, conf))
            elif conf > 0.5:
                confidence_levels['medium'].append((i, conf))
            else:
                confidence_levels['low'].append((i, conf))

        print(f"\n[ç½®ä¿¡åº¦åˆ†å¸ƒ]")
        print(f"  é«˜ç½®ä¿¡åº¦ (>0.8): {len(confidence_levels['high'])} ä¸ª "
              f"[{len(confidence_levels['high'])/total_detections*100:.1f}%]" if total_detections > 0 else "")
        print(f"  ä¸­ç­‰ç½®ä¿¡åº¦ (0.5-0.8): {len(confidence_levels['medium'])} ä¸ª "
              f"[{len(confidence_levels['medium'])/total_detections*100:.1f}%]" if total_detections > 0 else "")
        print(f"  ä½ç½®ä¿¡åº¦ (<0.5): {len(confidence_levels['low'])} ä¸ª "
              f"[{len(confidence_levels['low'])/total_detections*100:.1f}%]" if total_detections > 0 else "")

        # è¯¦ç»†æ£€æµ‹åˆ—è¡¨
        print(f"\n[æ£€æµ‹è¯¦æƒ…]")
        detections = list(zip(range(len(boxes)), boxes, confidences, classIDs))
        detections.sort(key=lambda x: x[2], reverse=True)  # æŒ‰ç½®ä¿¡åº¦æ’åº

        for idx, box, conf, cid in detections:
            class_name = labels[cid]
            quality = self._get_quality_label(conf)
            print(f"  #{idx+1}: {class_name:<15} ç½®ä¿¡åº¦: {conf:.2%}  {quality}")

        # è´¨é‡è¯„ä¼°
        print(f"\n[è´¨é‡è¯„ä¼°]")
        self._assess_quality(confidence_levels, total_detections, avg_confidence)

        # ä¼˜åŒ–å»ºè®®
        print(f"\n[ä¼˜åŒ–å»ºè®®]")
        self._provide_recommendations(confidence_levels, total_detections, avg_confidence, inference_time)

        # ä¿å­˜åˆ°å†å²
        result_record = {
            'timestamp': datetime.now().isoformat(),
            'total_detections': total_detections,
            'avg_confidence': avg_confidence,
            'class_distribution': class_distribution,
            'confidence_levels': {
                'high': len(confidence_levels['high']),
                'medium': len(confidence_levels['medium']),
                'low': len(confidence_levels['low'])
            },
            'inference_time': inference_time
        }
        self.results_history.append(result_record)

        print("\n" + "=" * 60)

        return result_record

    def _get_quality_label(self, confidence):
        """è·å–è´¨é‡æ ‡ç­¾"""
        if confidence > 0.8:
            return "âœ… ä¼˜ç§€"
        elif confidence > 0.6:
            return "âœ… è‰¯å¥½"
        elif confidence > 0.5:
            return "âš ï¸  ä¸€èˆ¬"
        else:
            return "âŒ è¾ƒå·®"

    def _assess_quality(self, confidence_levels, total, avg_conf):
        """è¯„ä¼°æ•´ä½“è´¨é‡"""
        high_ratio = len(confidence_levels['high']) / total if total > 0 else 0
        low_ratio = len(confidence_levels['low']) / total if total > 0 else 0

        print("  ä¼˜ç‚¹:")
        if high_ratio > 0.5:
            print(f"    âœ… è¶…è¿‡ä¸€åŠçš„æ£€æµ‹å…·æœ‰é«˜ç½®ä¿¡åº¦ ({high_ratio*100:.1f}%)")
        if avg_conf > 0.7:
            print(f"    âœ… å¹³å‡ç½®ä¿¡åº¦è¾ƒé«˜ ({avg_conf:.2%})")
        if low_ratio == 0:
            print(f"    âœ… æ²¡æœ‰ä½ç½®ä¿¡åº¦æ£€æµ‹ï¼Œè´¨é‡ç¨³å®š")

        print("\n  é—®é¢˜:")
        if high_ratio < 0.3:
            print(f"    âŒ é«˜ç½®ä¿¡åº¦æ£€æµ‹è¾ƒå°‘ ({high_ratio*100:.1f}%)ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æ¨¡å‹æˆ–å‚æ•°")
        if avg_conf < 0.6:
            print(f"    âŒ å¹³å‡ç½®ä¿¡åº¦åä½ ({avg_conf:.2%})")
        if low_ratio > 0.2:
            print(f"    âŒ å­˜åœ¨è¾ƒå¤šä½ç½®ä¿¡åº¦æ£€æµ‹ ({low_ratio*100:.1f}%)ï¼Œå¯èƒ½æœ‰è¯¯æ£€")

        if high_ratio >= 0.5 and low_ratio == 0:
            print("\n  æ€»ä½“è¯„ä»·: ğŸŒŸ æ£€æµ‹è´¨é‡ä¼˜ç§€")
        elif high_ratio >= 0.3 and avg_conf >= 0.6:
            print("\n  æ€»ä½“è¯„ä»·: âœ… æ£€æµ‹è´¨é‡è‰¯å¥½")
        elif avg_conf >= 0.5:
            print("\n  æ€»ä½“è¯„ä»·: âš ï¸  æ£€æµ‹è´¨é‡ä¸€èˆ¬ï¼Œæœ‰æ”¹è¿›ç©ºé—´")
        else:
            print("\n  æ€»ä½“è¯„ä»·: âŒ æ£€æµ‹è´¨é‡è¾ƒå·®ï¼Œéœ€è¦ä¼˜åŒ–")

    def _provide_recommendations(self, confidence_levels, total, avg_conf, inference_time):
        """æä¾›ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # ç½®ä¿¡åº¦é˜ˆå€¼å»ºè®®
        low_ratio = len(confidence_levels['low']) / total if total > 0 else 0
        high_ratio = len(confidence_levels['high']) / total if total > 0 else 0

        if low_ratio > 0.2:
            recommendations.append(
                "ğŸ“Œ æ£€æµ‹åˆ°è¾ƒå¤šä½ç½®ä¿¡åº¦ç»“æœï¼Œå»ºè®®æé«˜ CONFIDENCE é˜ˆå€¼åˆ° 0.6-0.7 ä»¥å‡å°‘è¯¯æ£€"
            )
        elif high_ratio < 0.3 and avg_conf < 0.6:
            recommendations.append(
                "ğŸ“Œ æ•´ä½“ç½®ä¿¡åº¦åä½ï¼Œå»ºè®®æ£€æŸ¥è¾“å…¥å›¾åƒè´¨é‡æˆ–è€ƒè™‘ä½¿ç”¨æ›´é«˜è´¨é‡çš„æ¨¡å‹"
            )

        # æ€§èƒ½ä¼˜åŒ–å»ºè®®
        if inference_time:
            if inference_time > 0.3:
                recommendations.append(
                    f"âš¡ æ¨ç†æ—¶é—´è¾ƒé•¿ ({inference_time*1000:.0f}ms)ï¼Œå»ºè®®ï¼š"
                    f"\n     - å‡å°è¾“å…¥å°ºå¯¸ (å¦‚ä» 416x416 åˆ° 320x320)"
                    f"\n     - ä½¿ç”¨ GPU åŠ é€Ÿ"
                    f"\n     - è€ƒè™‘ä½¿ç”¨æ›´è½»é‡çš„æ¨¡å‹"
                )
            elif inference_time > 0.1:
                recommendations.append(
                    f"âš¡ æ¨ç†æ—¶é—´é€‚ä¸­ ({inference_time*1000:.0f}ms)ï¼Œå¦‚éœ€æå‡ï¼š"
                    f"\n     - å¯å°è¯•ä½¿ç”¨ GPU åŠ é€Ÿ"
                    f"\n     - æˆ–é€‚å½“å‡å°è¾“å…¥å°ºå¯¸"
                )
            else:
                recommendations.append(
                    f"âœ… æ¨ç†é€Ÿåº¦ä¼˜ç§€ ({inference_time*1000:.0f}ms)ï¼Œæ»¡è¶³å®æ—¶æ£€æµ‹éœ€æ±‚"
                )

        # NMS å»ºè®®
        if total > 10 and avg_conf > 0.7:
            recommendations.append(
                "ğŸ“Œ æ£€æµ‹æ•°é‡è¾ƒå¤šä¸”ç½®ä¿¡åº¦é«˜ï¼Œå¦‚æœ‰é‡å¤æ¡†ï¼Œå¯é€‚å½“é™ä½ THRESHOLD (NMS) åˆ° 0.3"
            )

        # è¾“å‡ºå»ºè®®
        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        else:
            print("  âœ… å½“å‰å‚æ•°é…ç½®è‰¯å¥½ï¼Œæ— éœ€è°ƒæ•´")

    def compare_with_history(self):
        """ä¸å†å²è®°å½•å¯¹æ¯”"""
        if len(self.results_history) < 2:
            print("å†å²è®°å½•ä¸è¶³ï¼Œæ— æ³•å¯¹æ¯”")
            return

        print("\n" + "=" * 60)
        print("å†å²å¯¹æ¯”åˆ†æ")
        print("=" * 60)

        latest = self.results_history[-1]
        previous = self.results_history[-2]

        print(f"\næœ€æ–° vs ä¸Šæ¬¡:")
        print(f"  æ£€æµ‹æ•°é‡: {latest['total_detections']} vs {previous['total_detections']} "
              f"({'â†‘' if latest['total_detections'] > previous['total_detections'] else 'â†“'})")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {latest['avg_confidence']:.2%} vs {previous['avg_confidence']:.2%} "
              f"({'â†‘' if latest['avg_confidence'] > previous['avg_confidence'] else 'â†“'})")

        if latest['inference_time'] and previous['inference_time']:
            print(f"  æ¨ç†æ—¶é—´: {latest['inference_time']*1000:.2f}ms vs {previous['inference_time']*1000:.2f}ms "
                  f"({'â†“ æ›´å¿«' if latest['inference_time'] < previous['inference_time'] else 'â†‘ æ›´æ…¢'})")

    def export_report(self, filename="detection_report.json"):
        """å¯¼å‡ºåˆ†ææŠ¥å‘Š"""
        report = {
            'analysis_time': datetime.now().isoformat(),
            'total_analyses': len(self.results_history),
            'history': self.results_history
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nåˆ†ææŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {filename}")


# ä¾¿æ·å‡½æ•°
def quick_analyze(boxes, confidences, classIDs, labels, inference_time=None):
    """å¿«é€Ÿåˆ†ææ£€æµ‹ç»“æœ"""
    analyzer = DetectionAnalyzer()
    return analyzer.analyze_detection_result(boxes, confidences, classIDs, labels, inference_time)


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("YOLO æ£€æµ‹ç»“æœåˆ†æå·¥å…·")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("1. åœ¨æ‚¨çš„æ£€æµ‹è„šæœ¬ä¸­å¯¼å…¥æ­¤æ¨¡å—:")
    print("   from detection_analyzer import DetectionAnalyzer, quick_analyze")
    print("\n2. åœ¨æ£€æµ‹å®Œæˆåè°ƒç”¨åˆ†æå‡½æ•°:")
    print("   analyzer = DetectionAnalyzer()")
    print("   analyzer.analyze_detection_result(boxes, confidences, classIDs, labels, inference_time)")
    print("\n3. æˆ–ä½¿ç”¨å¿«é€Ÿåˆ†æ:")
    print("   quick_analyze(boxes, confidences, classIDs, labels, inference_time)")

    # æ¨¡æ‹Ÿç¤ºä¾‹
    print("\n" + "=" * 60)
    print("è¿è¡Œç¤ºä¾‹åˆ†æ...")
    print("=" * 60)

    # æ¨¡æ‹Ÿæ£€æµ‹æ•°æ®
    labels = ['person', 'car', 'dog', 'bicycle']
    boxes = [[10, 20, 100, 200], [150, 50, 80, 120], [300, 100, 50, 80]]
    confidences = [0.85, 0.62, 0.48]
    classIDs = [0, 1, 0]
    inference_time = 0.15

    quick_analyze(boxes, confidences, classIDs, labels, inference_time)
